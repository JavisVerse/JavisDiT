## JavisBench: A Challenging Benchmark for for Joint Audio-Video Generation (JAVG) Evaluation

As released in [HuggingFace](https://huggingface.co/datasets/JavisVerse/JavisBench),
JavisBench is a comprehensive and challenging benchmark for evaluating **text-to-audio-video generation** models.  
It covers multiple aspects of generation quality, semantic alignment, and temporal synchrony, enabling thorough assessment in both controlled and real-world scenarios.

### 1. Data Composition

JavisBench integrates test data from two sources:

1. Reusage of **existing benchmarks**, including [Landscape](https://github.com/researchmm/MM-Diffusion#Test), [AIST++](https://github.com/researchmm/MM-Diffusion#Test), [FAVDBench](https://github.com/OpenNLPLab/FAVDBench).
2. Newly collected **YouTube videos** (collected between **June 2024 – Dec 2024** to avoid data leakage).

In total, **JavisBench** contains **10,140** audio-video samples with annotated captions and various attributes.
In particular, the task scenario in JavisBench covers **5 aspects and 19 specific categories**, designed to comprehensively evaluate JAVG models in real-world scenarios.

<details> 
<summary> Details of JavisBench Taxonomy</summary>

| Aspect | Category | Description and Examples |
|---|---|---|
| Event Scenario | Natural Scenario | Scenes dominated by natural environments with minimal human interference, such as forests, oceans, and mountains. |
|  | Urban Scenario | Outdoor spaces shaped by human activity, including cities, villages, streets, and parks. |
|  | Living Scenario | Indoor environments where daily human activities occur, like houses, schools, and shopping malls. |
|  | Industrial Scenario | Work-oriented spaces related to industrial or energy activities, such as factories, construction sites, and mines. |
|  | Virtual Scenario | Imaginative or abstract settings, including virtual worlds, sci-fi cities, and artistic installations. |
| Visual Style | Camera Shooting | Filmed with handheld, fixed, or drone cameras, including slow-motion footage. |
|  | 2D-Animate | Styles like hand-drawn animation, flat animation, cartoon styles, or watercolor illustrations. |
|  | 3D-Animate | Photorealistic styles, sci-fi/magical effects, CG (Computer Graphics), or steam- punk aesthetics. |
| Sound Type | Ambient Sounds | Sounds that occur naturally in the environment, including both natural and human-influenced surroundings. This category includes sounds like wind, rain, water flow, animal sounds, human activity (e.g., traffic, construction), and urban noise. |
|  | Biological Sounds | Sounds produced by living creatures (e.g.animals, birds). This includes vocal- izations such as barking, chirping, growling, as well as non-vocal human sounds like heartbeat, and other physical noises. |
|  | Mechanical Sounds | Sounds generated by man-made machines, devices, or mechanical processes. This includes the noise of engines, motors, appliances, and any mechanical or electronic noise. This category also includes malfunction sounds (e.g., malfunc- tioning machinery or alarms). |
|  | Musical Sounds | Sounds related to music or musical performance, including both human- generated and instrument-generated sounds and melodies. This category covers singing, instrumental performances, as well as background music used in vari- ous media formats. |
|  | Speech Sounds | Sounds generated from human speech, whether in conversation, dialogue, public speeches, debates, interviews, or monologues. This category specifically covers linguistic communication in various contexts, whether formal, informal, or contentious. |
| Spatial Composition | Single Subject | There is only one primary object or source producing sound in the scene. |
|  | Multiple Subject | There are multiple primary objects that (or potentially can) make sounds in the scene. |
|  | Off-screen Sound | The source of the sound is not visible in the scene but logically exists (e.g., a car engine outside the camera view). |
| Temporal Composition | Single Event | The audio contains only one event, with no overlapping sounds. For example, “a single dog barking without background noise.” |
|  | Sequential Events | There are multiple events occurring sequentially, with no overlap. For example, “the applause begins after the music performance ends.” |
|  | Simultaneous Events | Multiple audio sources are present simultaneously, such as “a person speaking while music plays in the background.” |

</details>

<br>

To support faster evaluation, we also provide a smaller-scale **JavisBench-mini**, which contains **1,000** samples randomly sampled from the original dataset.


:warning: **NOTE**: YouTube raw audio-video data is not released due to copyright restrictions. Instead, we provide pre-extracted audio-video features for FVD/KVD/FAD evaluation (will be introduced below). For other metrics, raw audio-video data is not required — only input textual captions and generated audio-video pairs from your model are needed.


### 2. Evaluation Metrics

We evaluate JAVG models from **4** complementary perspectives: (1) Audio/Video Quality, (2) Text to Audio/Video Consistency, (3) Audio-Video Consistency, and (4) Audio-Video Synchrony.


<details> 
<summary> Clik to expand: (1) Audio/Video Quality </summary>

Measures on the perceptual quality of the generated audio and video.

* **Fréchet Video Distance (FVD)**
  Formula:

  $\mathrm{FVD} = \|\mu_r - \mu_g\|_2^2 + \mathrm{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2})$

  where $(\mu_r, \Sigma_r)$ and $(\mu_g, \Sigma_g)$ are the mean and covariance of **real** and **generated** video features extracted by a pretrained video feature encoder (e.g., [I3D](https://arxiv.org/pdf/1705.07750)).
  **Lower is better**, indicating the generated video distribution is closer to the real one.

* **Kernel Video Distance (KVD)**
  Similar to FVD, but estimates distribution differences via a kernel-based method (Kernel Inception Distance style), which is more stable on smaller datasets; **lower is better**.

* **Fréchet Audio Distance (FAD)**
  Same concept as FVD, but computed on **audio** features extracted by a pretrained audio model (e.g., [AudioClip](https://arxiv.org/pdf/2106.13043)).
  Measures distribution distance between generated and real audio; **lower is better**.

</details> 


<details> 
<summary> Clik to expand: (2) Text to Audio/Video Consistency </summary>


Evaluates how well the generated audio and video semantically match the input text description.

* **[ImageBind](https://github.com/facebookresearch/ImageBind) Similarity**

  * **Text–Video**: Encode text $t$ and video $v$ into a shared embedding space and compute cosine similarity:

    $\mathrm{sim}(t, v) = \frac{f_{\mathrm{text}}(t) \cdot f_{\mathrm{video}}(v)}{\|f_{\mathrm{text}}(t)\| \cdot \|f_{\mathrm{video}}(v)\|}$

  * **Text–Audio**: Same process but with the audio encoder $f_{\mathrm{audio}}$.

* **[CLIP](https://github.com/openai/CLIP) Similarity** (Text–Video)
  Uses CLIP to compute semantic similarity between text and video (video frames are sampled, encoded, and averaged).

* **[CLAP](https://github.com/LAION-AI/CLAP) Similarity** (Text–Audio)
  Uses CLAP to compute semantic similarity between text and audio.

</details> 

<details> 
<summary> Clik to expand: (3) Audio-Video Consistency </summary>

Measures the semantic alignment between generated audio and generated video.

* **ImageBind (Video–Audio)**:
  Encodes both modalities into the same space and computes cosine similarity between video and audio features.

* **[CAVP](https://github.com/luosiallen/Diff-Foley) (Cross-Audio-Video Pretraining)**:
  A dedicated pretrained model for cross-modal matching; higher similarity indicates better semantic correspondence.

* **[AVHScore](https://arxiv.org/pdf/2404.14381) (Audio-Visual Harmony Score)**:
  Introduced in [TAVGBench](https://arxiv.org/pdf/2404.14381) as a way to quantify how well the generated audio and video align semantically in a shared embedding space .
  It is defined by computing the cosine similarity between **each video frame** and the **entire audio**, then averaging across all frames:

  $\text{AVHScore} = \frac{1}{N} \sum_{i=1}^{N} \cos\bigl(f_{\mathrm{frame}}(v_i),\; f_{\mathrm{audio}}(a)\bigr)$

  A higher AVHScore indicates stronger audio–video semantic consistency.

</details> 


<details> 
<summary> Clik to expand: (4) Audio-Video Synchrony </summary>

- **JavisScore**:
  A new metric we propose to measure temporal synchrony between audio and video. The core idea is using a sliding window along the temporal axis to split the audio-video pair into short segments. For each segment, compute cross-modal similarity (e.g., with [ImageBind]((https://github.com/facebookresearch/ImageBind) )) and take the mean score:

  $\mathrm{JavisScore} = \frac{1}{N} \sum_{i=1}^{N} \sigma(a_i, v_i) , \quad \sigma(v_i,a_i) = \frac{1}{k} \sum_{j=1}^{k} \mathop{\text{top-}k}\limits_{\min} \{ \cos\left(E_v(v_{i,j}), E_a(a_{i})\right) \}$

- **[AV-Align](https://arxiv.org/pdf/2309.16429)**:
  Although we did not report this metric in the paper (due to its inefficacy in evaluation complex audio-video synchrony), we also provide a reference implementation in the codebase for potential future research.

  Given energy peaks detected in both audio (estimated by audio onsets) and video (estimated by optical flow):
  
  $\mathrm{P}_a = \{t_{a,1}, t_{a,2}, \dots\},\quad
  \mathrm{P}_v = \{t_{v,1}, t_{v,2}, \dots\}$
  
  Then evaluate how often peaks align within a short temporal window (e.g., ±3 frames).

  $\text{AV-Align} = \frac{1}{|\mathrm{P}_a| + |\mathrm{P}_v|}
  \left( \sum_{t_a \in \mathrm{P}_a}
         \mathbb{1}_{\exists\, t_v \in \mathrm{P}_v\,:\,|t_a - t_v| \leq \tau}
       + \sum_{t_v \in \mathrm{P}_v}
         \mathbb{1}_{\exists\, t_a \in \mathrm{P}_a\,:\,|t_v - t_a| \leq \tau}
  \right)$

  * $\tau$ is the temporal tolerance window (e.g., 3 frames).
  * $\mathbb{1}_{\cdot}$ is the indicator function—1 if a match exists within the window, otherwise 0.
  * Higher scores indicate better alignment in both directions.

</details> 


### 3. Evaluation

Install the evaluation dependencies:

```bash
cd /path/to/JavisDiT

pip install -r requirements/requirements-eval.txt
```

Assume your generated results are saved under:

```bash
samples/JavisBench/sample_0000.mp4  # or JavisBench-mini
samples/JavisBench/sample_0000.wav
samples/JavisBench/sample_0001.mp4
samples/JavisBench/sample_0001.wav
...
```

From the root directory of the **JavisDiT** project,
download the meta file and data of [JavisBench](https://huggingface.co/datasets/JavisVerse/JavisBench), and put them into `data/eval/`:

```bash
cd /path/to/JavisDiT
mkdir -p data/eval

huggingface-cli download --repo-type dataset JavisVerse/JavisBench --local-dir data/eval/JavisBench
```

Then, run evaluation:

```bash
MAX_FRAMES=16
IMAGE_SIZE=224
MAX_AUDIO_LEN_S=4.0

# Params to calculate JavisScore
WINDOW_SIZE_S=2.0
WINDOW_OVERLAP_S=1.5

METRICS="all" 
RESULTS_DIR="./evaluation_results"

DATASET="JavisBench"  # or JavisBench-mini
INPUT_FILE="data/eval/JavisBench/${DATASET}.csv"
FVD_AVCACHE_PATH="data/eval/JavisBench/cache/fvd_fad/${DATASET}-vanilla-max4s.pt"
INFER_DATA_DIR="samples/${DATASET}"

python -m eval.javisbench.main \
  --input_file "${INPUT_FILE}" \
  --infer_data_dir "${INFER_DATA_DIR}" \
  --output_file "${RESULTS_DIR}/${DATASET}.json" \
  --max_frames ${MAX_FRAMES} \
  --image_size ${IMAGE_SIZE} \
  --max_audio_len_s ${MAX_AUDIO_LEN_S} \
  --window_size_s ${WINDOW_SIZE_S} \
  --window_overlap_s ${WINDOW_OVERLAP_S} \
  --fvd_avcache_path ${FVD_AVCACHE_PATH} \
  --metrics ${METRICS}
```

The results will be displayed in terminal and saved in `./evaluation_results`.


## Citation

If you use JavisBench in your research, please cite:

```bibtex
@inproceedings{liu2025javisdit,
      title={JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization}, 
      author={Kai Liu and Wei Li and Lai Chen and Shengqiong Wu and Yanhao Zheng and Jiayi Ji and Fan Zhou and Rongxin Jiang and Jiebo Luo and Hao Fei and Tat-Seng Chua},
      booktitle={arxiv},
      year={2025}, 
}
```
